{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dda0b0efc8ba86e81ec4</td>\n",
       "      <td>What are interesting facts about Microsoft his...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dc708b74a108d0fc0ad9</td>\n",
       "      <td>What are those things which are not gonna happ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06a27ec5d82dacd8bfe0</td>\n",
       "      <td>What should I know to avoid being \"upsold\" whe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00cbb6b17e3ceb7c5358</td>\n",
       "      <td>How I add any account with payment bank?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7c304888973a701585a0</td>\n",
       "      <td>Which Multi level marketing products are actua...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>4bd96088d0b5f0f2c4f4</td>\n",
       "      <td>How is CSE at VIT Chennai?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>e80edbfc086f7125940f</td>\n",
       "      <td>How can we prevent a holocaust by robots, AI, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>1506dfad6bd340782a1f</td>\n",
       "      <td>How can I help a student remember key steps an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>b56c60fd407f2f85553c</td>\n",
       "      <td>What is the difference between lace closure &amp; ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>a1b32d315c2782cdbcc3</td>\n",
       "      <td>What happens when you look into a broken mirror?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         qid  \\\n",
       "0       dda0b0efc8ba86e81ec4   \n",
       "1       dc708b74a108d0fc0ad9   \n",
       "2       06a27ec5d82dacd8bfe0   \n",
       "3       00cbb6b17e3ceb7c5358   \n",
       "4       7c304888973a701585a0   \n",
       "...                      ...   \n",
       "999995  4bd96088d0b5f0f2c4f4   \n",
       "999996  e80edbfc086f7125940f   \n",
       "999997  1506dfad6bd340782a1f   \n",
       "999998  b56c60fd407f2f85553c   \n",
       "999999  a1b32d315c2782cdbcc3   \n",
       "\n",
       "                                            question_text  target  \n",
       "0       What are interesting facts about Microsoft his...       0  \n",
       "1       What are those things which are not gonna happ...       0  \n",
       "2       What should I know to avoid being \"upsold\" whe...       0  \n",
       "3                How I add any account with payment bank?       0  \n",
       "4       Which Multi level marketing products are actua...       0  \n",
       "...                                                   ...     ...  \n",
       "999995                         How is CSE at VIT Chennai?       0  \n",
       "999996  How can we prevent a holocaust by robots, AI, ...       0  \n",
       "999997  How can I help a student remember key steps an...       0  \n",
       "999998  What is the difference between lace closure & ...       0  \n",
       "999999   What happens when you look into a broken mirror?       0  \n",
       "\n",
       "[1000000 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../datasets/train_df.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word tokenizing: ['Life', 'is', 'beautiful', 'so', 'Enjoy', 'everymoment', 'you', 'have', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/btv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    return nltk.word_tokenize(sent)\n",
    "\n",
    "print(\"word tokenizing:\",word_tokenize(\"Life is beautiful so Enjoy everymoment you have.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['life',\n",
       " 'is',\n",
       " 'beautiful',\n",
       " 'so',\n",
       " 'enjoy',\n",
       " 'everymoment',\n",
       " 'you',\n",
       " 'have',\n",
       " 'runners',\n",
       " 'run',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'win']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "def regex_word_tokenizer(sent):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    sample_word_tokens = tokenizer.tokenize(sent)\n",
    "    sample_word_tokens = [word.lower() for word in sample_word_tokens]\n",
    "    return sample_word_tokens\n",
    "\n",
    "words = regex_word_tokenizer(str(\"Life is beautiful so Enjoy everymoment you have. Runners run hard to win\"))\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['life', 'beautiful', 'enjoy', 'everymoment', 'runners', 'run', 'hard', 'win']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def stop_words_removal(words):\n",
    "    stop_words = [word.lower() for word in stopwords.words('english')]\n",
    "    word_tokens = [word for word in words if word.lower() not in stop_words]\n",
    "    return word_tokens\n",
    "\n",
    "stop_words_removal(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['life', 'beautiful', 'enjoy', 'everymoment', 'runner', 'run', 'hard', 'win']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def Lemmatizer(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return lemmatized_words\n",
    "\n",
    "Lemmatizer(stop_words_removal(words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['life', 'beauti', 'enjoy', 'everymo', 'runner', 'run', 'hard', 'win']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stemmer(words):\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = [ps.stem(w) for w in words]\n",
    "    return stemmed_words\n",
    "\n",
    "stemmer(stop_words_removal(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'life': True,\n",
       " 'beautiful': True,\n",
       " 'enjoy': True,\n",
       " 'everymoment': True,\n",
       " 'runner': True,\n",
       " 'run': True,\n",
       " 'hard': True,\n",
       " 'win': True}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_sentence(sent):\n",
    "    tokens = regex_word_tokenizer(sent)\n",
    "    tokens = stop_words_removal(tokens)\n",
    "    tokens = Lemmatizer(tokens)\n",
    "    return ({word: True for word in tokens})\n",
    "\n",
    "format_sentence(\"Life is beautiful so Enjoy everymoment you have. Runners run hard to win\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = []\n",
    "neg = []\n",
    "\n",
    "\n",
    "for index,row in df.iterrows(): \n",
    "    if(row['target']==1):\n",
    "        neg.append([format_sentence(row['question_text']), 1])\n",
    "    else:\n",
    "        pos.append([format_sentence(row['question_text']), 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pos[:int((.9)*len(pos))] + neg[:int((.9)*len(neg))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pos[int((.1)*len(pos)):] + neg[int((.1)*len(neg)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "            islamophobic = True                1 : 0      =    156.7 : 1.0\n",
      "                   apist = True                1 : 0      =    136.5 : 1.0\n",
      "                  drumpf = True                1 : 0      =    116.2 : 1.0\n",
      "                castrate = True                1 : 0      =    115.9 : 1.0\n",
      "                   aping = True                1 : 0      =    112.9 : 1.0\n",
      "                butthurt = True                1 : 0      =    106.1 : 1.0\n",
      "               castrated = True                1 : 0      =     98.1 : 1.0\n",
      "                shitting = True                1 : 0      =     96.0 : 1.0\n",
      "                   wumao = True                1 : 0      =     96.0 : 1.0\n",
      "                   moron = True                1 : 0      =     95.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8404188888888889\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.util import accuracy\n",
    "\n",
    "print(accuracy(classifier, test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
