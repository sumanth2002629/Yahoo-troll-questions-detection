{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../datasets/train_df.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regexp Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "def regex_word_tokenizer(sent):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    sample_word_tokens = tokenizer.tokenize(sent)\n",
    "    sample_word_tokens = [word.lower() for word in sample_word_tokens]\n",
    "    return sample_word_tokens\n",
    "\n",
    "words = regex_word_tokenizer(str(\"Life is beautiful so Enjoy everymoment you have. Runners run hard to win\"))\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words_removal(words):\n",
    "    stop_words = [word.lower() for word in stopwords.words('english')]\n",
    "    word_tokens = [word for word in words if word.lower() not in stop_words]\n",
    "    return word_tokens\n",
    "\n",
    "stop_words_removal(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def Lemmatizer(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return lemmatized_words\n",
    "\n",
    "Lemmatizer(stop_words_removal(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sentence(sent):\n",
    "    \n",
    "    # sentence = spellchecker(sent)\n",
    "    # tokens = word_tokenize(str(sent))\n",
    "    tokens = regex_word_tokenizer(str(sent))\n",
    "    # tokens = stop_words_removal(tokens)\n",
    "    # tokens = Lemmatizer(tokens)\n",
    "\n",
    "    sentence = \"\"\n",
    "    for word in tokens:\n",
    "        sentence += word + \" \"\n",
    "    return sentence\n",
    "\n",
    "format_sentence(\"Life havv prpblems beautiful so Enjoy every moment you have. Runners run hard to win caring careful careless\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "\n",
    "for index,row in df.iterrows(): \n",
    "    if(row['target']==1):\n",
    "        X.append(format_sentence(row['question_text']))\n",
    "        Y.append(1)\n",
    "    else:\n",
    "        X.append(format_sentence(row['question_text']))\n",
    "        Y.append(0)\n",
    "\n",
    "X = pd.DataFrame(X,columns=[\"text\"])\n",
    "Y = pd.DataFrame(Y)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X[\"text\"],Y,test_size=0.01,random_state = 42)\n",
    "print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext as ft\n",
    "\n",
    "# Load the pretrained model\n",
    "ft_model = ft.load_model(\"../pretrained_model/lid.176.bin\")\n",
    "\n",
    "def fasttext_language_predict(text, model = ft_model):\n",
    "\n",
    "  text = text.replace('\\n', \" \")\n",
    "  prediction = model.predict([text])\n",
    "\n",
    "  return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "en_0 = 0\n",
    "nen_0 = 0\n",
    "en_1 = 0\n",
    "nen_1 = 0\n",
    "\n",
    "for index,row in df.iterrows():\n",
    "    if(row['target']==1):\n",
    "        if fasttext_language_predict(format_sentence(row['question_text']))[0][0][0][9:]==\"en\" and fasttext_language_predict(format_sentence(row['question_text']))[1][0][0]>0.7:\n",
    "            en_1+=1\n",
    "        else:\n",
    "            if index <=1000:\n",
    "                print(format_sentence(row['question_text']),1)\n",
    "            nen_1+=1\n",
    "\n",
    "    else:\n",
    "        if fasttext_language_predict(format_sentence(row['question_text']))[0][0][0][9:]==\"en\" and fasttext_language_predict(format_sentence(row['question_text']))[1][0][0]>0.7:\n",
    "            en_0+=1\n",
    "        else:\n",
    "            if index <=1000:\n",
    "                print(format_sentence(row['question_text']))\n",
    "            nen_0+=1\n",
    "\n",
    "    if(row['target']==1):\n",
    "        X.append(format_sentence(row['question_text']))\n",
    "        Y.append(1)\n",
    "    else:\n",
    "        # if fasttext_language_predict(format_sentence(row['question_text']))[0][0][0][9:]==\"en\" and fasttext_language_predict(format_sentence(row['question_text']))[1][0][0]>0.7:\n",
    "        X.append(format_sentence(row['question_text']))\n",
    "        Y.append(0)\n",
    "\n",
    "X = pd.DataFrame(X,columns=[\"text\"])\n",
    "Y = pd.DataFrame(Y)\n",
    "\n",
    "print(en_1,nen_1,en_0,nen_0)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# import numpy as np\n",
    "# rus = RandomUnderSampler(random_state=42,sampling_strategy=0.20)\n",
    "# X_data = X[\"text\"].values.reshape(-1,1)\n",
    "# X_train, y_train= rus.fit_resample(X_data,Y)\n",
    "\n",
    "# print(len(X_train),len(y_train))\n",
    "\n",
    "# ones = 0\n",
    "# zeros = 0\n",
    "# y_train = np.array(y_train)\n",
    "# for i in range(0,len(y_train)):\n",
    "#     if not y_train[i]:\n",
    "#         zeros+=1\n",
    "#     else:\n",
    "#         ones+=1\n",
    "# print(ones,zeros)\n",
    "\n",
    "# X_train,X_test,Y_train,Y_test = train_test_split(X_train,y_train,random_state = 42)\n",
    "# X_train = X_train.flatten()\n",
    "# X_test = X_test.flatten()\n",
    "# Y_train = Y_train.flatten()\n",
    "# Y_test = Y_test.flatten()\n",
    "# print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "# tfidf_vectorizer=TfidfVectorizer(use_idf=True,max_df=0.25, sublinear_tf=True)\n",
    "tfidf_vectorizer=TfidfVectorizer(sublinear_tf=True,ngram_range= (1,3),smooth_idf=False,min_df=0.1)\n",
    "vect=tfidf_vectorizer.fit_transform(X_train) \n",
    "vect=tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "# vect.get_feature_names()[::1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vect = CountVectorizer().fit(X_train)\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vect = CountVectorizer().fit(X_train)\n",
    "\n",
    "##TFIDF\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "# tfidf_vectorizer=TfidfVectorizer(use_idf=True) \n",
    "# vect=tfidf_vectorizer.fit_transform(X_train) \n",
    "# vect=tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "# vect.get_feature_names()[::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorised = vect.transform(X_train)\n",
    "X_train_vectorised\n",
    "# print(len(Y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=3, random_state=0)\n",
    "clf = LogisticRegression(penalty = \"l2\",max_iter = 1000000,random_state = 42,solver=\"lbfgs\",class_weight={0:1,1:1.11},dual=False,intercept_scaling=1000)\n",
    "\n",
    "clf.fit(X_train_vectorised,np.array(Y_train[0]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# import numpy as np\n",
    "# svclassifier = SVC(kernel='sigmoid')\n",
    "# svclassifier.fit(X_train_vectorised,np.array(Y_train[0]))\n",
    "# y_pred = svclassifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([])\n",
    "probas = clf.predict_proba(vect.transform(X_test))\n",
    "y_pred = []\n",
    "for i in range(len(probas)):\n",
    "  if(probas[i][0]<0.8):\n",
    "    y_pred.append(1)\n",
    "  else:\n",
    "    y_pred.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cf_matrix = confusion_matrix(np.array(Y_test[0]), y_pred)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(np.array(Y_test[0]),y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../datasets/test_df.csv\")\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([])\n",
    "probas = clf.predict_proba(vect.transform(test_df[\"question_text\"]))\n",
    "y_pred = []\n",
    "for i in range(len(probas)):\n",
    "  if(probas[i][0]<0.8):\n",
    "    y_pred.append(1)\n",
    "  else:\n",
    "    y_pred.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop([\"question_text\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"target\"]=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('../results/lr_withProb0.80_classWeight_withLang_maxdf0.25.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
